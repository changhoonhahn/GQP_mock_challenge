{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_desi_complexdust_speculator_wavebins",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPaX+Ksl5FS66HIT2oJ5Y3Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changhoonhahn/gqp_mc/blob/master/nb/training_desi_complexdust_speculator_wavebins.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_IHasbaJ3bu",
        "outputId": "a46131b9-fdaf-4f6e-ecd8-2df01ad4b06c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxG99o_9J7nF",
        "outputId": "5e9fa8d0-2ab1-4d2f-b953-e7960a35fdff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/drive/My\\ Drive/speculator_fork"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/speculator_fork\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XMBUjiqKNOy"
      },
      "source": [
        "import os \n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from speculator import SpectrumPCA\n",
        "from speculator import Speculator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZJ8KvuRKOaC"
      },
      "source": [
        "# read DESI wavelength\n",
        "wave = np.load('wave_fsps.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WMWkF1upSyq"
      },
      "source": [
        "n_wave = 0\n",
        "if n_wave == 0: \n",
        "    wave_bin = (wave < 4500) \n",
        "elif n_wave == 1: \n",
        "    wave_bin = (wave >= 4500) & (wave < 6500) \n",
        "elif n_wave == 2: \n",
        "    wave_bin = (wave >= 6500) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZjNv5McKPaC"
      },
      "source": [
        "n_param = 10\n",
        "n_pcas  = 30 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mTGmpPGKVX1",
        "outputId": "d48b6cfd-d8c6-44d3-d8a5-bfa16adc4232",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# load trained PCA basis object\n",
        "print('training PCA bases') \n",
        "PCABasis = SpectrumPCA(\n",
        "        n_parameters=n_param,       # number of parameters\n",
        "        n_wavelengths=np.sum(wave_bin),       # number of wavelength values\n",
        "        n_pcas=n_pcas,              # number of pca coefficients to include in the basis \n",
        "        spectrum_filenames=None,  # list of filenames containing the (un-normalized) log spectra for training the PCA\n",
        "        parameter_filenames=[], # list of filenames containing the corresponding parameter values\n",
        "        parameter_selection=None) # pass an optional function that takes in parameter vector(s) and returns True/False for any extra parameter cuts we want to impose on the training sample (eg we may want to restrict the parameter ranges)\n",
        "PCABasis._load_from_file('DESI_complexdust.0_499.seed0.wave_bin%i.pca%i.hdf5' % (n_wave, n_pcas))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training PCA bases\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmeaPQSRP02T"
      },
      "source": [
        "_training_theta = np.load('DESI_complexdust.0_499.seed0.wave_bin%i.pca%i_parameters.npy' % (n_wave, n_pcas))\n",
        "_training_pca = np.load('DESI_complexdust.0_499.seed0.wave_bin%i.pca%i_pca.npy'% (n_wave, n_pcas))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7gmaXw8zGYa"
      },
      "source": [
        "N_train = int(4e6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bht7PTNwzFe7"
      },
      "source": [
        "training_theta = tf.convert_to_tensor(_training_theta.astype(np.float32)[:N_train,:])\n",
        "training_pca = tf.convert_to_tensor(_training_pca.astype(np.float32)[:N_train,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkOJ_36YP4zD",
        "outputId": "7f63983c-9128-4a4a-ac82-3c0cf448d527",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('training set size = %i' % training_pca.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training set size = 4000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKCvnjiRKZ6I",
        "outputId": "8bb837fc-4e72-4eec-ad7a-a1a6c07bafbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# train Speculator \n",
        "speculator = Speculator(\n",
        "        n_parameters=n_param, # number of model parameters \n",
        "        wavelengths=wave, # array of wavelengths\n",
        "        pca_transform_matrix=PCABasis.pca_transform_matrix,\n",
        "        parameters_shift=PCABasis.parameters_shift, \n",
        "        parameters_scale=PCABasis.parameters_scale, \n",
        "        pca_shift=PCABasis.pca_shift, \n",
        "        pca_scale=PCABasis.pca_scale, \n",
        "        spectrum_shift=PCABasis.spectrum_shift, \n",
        "        spectrum_scale=PCABasis.spectrum_scale, \n",
        "        n_hidden=[256, 256, 256], # network architecture (list of hidden units per layer)\n",
        "        restore=False, \n",
        "        optimizer=tf.keras.optimizers.Adam()) # optimizer for model training\n",
        "\n",
        "# cooling schedule\n",
        "lr = [1e-3, 1e-4, 1e-5, 1e-6]\n",
        "batch_size = [5000, 10000, 50000, 100000]#int(training_theta.shape[0])]\n",
        "gradient_accumulation_steps = [1, 1, 1, 1] # split the largest batch size into 10 when computing gradients to avoid memory overflow\n",
        "\n",
        "# early stopping set up\n",
        "patience = 20\n",
        "\n",
        "# train using cooling/heating schedule for lr/batch-size\n",
        "for i in range(len(lr)):\n",
        "    print('learning rate = ' + str(lr[i]) + ', batch size = ' + str(batch_size[i]))\n",
        "    # set learning rate\n",
        "    speculator.optimizer.lr = lr[i]\n",
        "\n",
        "    n_training = training_theta.shape[0]\n",
        "    # create iterable dataset (given batch size)\n",
        "    training_data = tf.data.Dataset.from_tensor_slices((training_theta, training_pca)).shuffle(n_training).batch(batch_size[i])\n",
        "\n",
        "    # set up training loss\n",
        "    training_loss   = [np.infty]\n",
        "    validation_loss = [np.infty]\n",
        "    best_loss       = np.infty\n",
        "    early_stopping_counter = 0\n",
        "\n",
        "    # loop over epochs\n",
        "    while early_stopping_counter < patience:\n",
        "\n",
        "        # loop over batches\n",
        "        for theta, pca in training_data:\n",
        "\n",
        "            # training step: check whether to accumulate gradients or not (only worth doing this for very large batch sizes)\n",
        "            if gradient_accumulation_steps[i] == 1:\n",
        "                loss = speculator.training_step(theta, pca)\n",
        "            else:\n",
        "                loss = speculator.training_step_with_accumulated_gradients(theta, pca, accumulation_steps=gradient_accumulation_steps[i])\n",
        "\n",
        "        # compute validation loss at the end of the epoch\n",
        "        validation_loss.append(speculator.compute_loss(training_theta, training_pca).numpy())\n",
        "\n",
        "        # early stopping condition\n",
        "        if validation_loss[-1] < best_loss:\n",
        "            best_loss = validation_loss[-1]\n",
        "            early_stopping_counter = 0\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "\n",
        "        if early_stopping_counter >= patience:\n",
        "            speculator.update_emulator_parameters()\n",
        "            speculator.save('_DESI_complexdust_model.Ntrain%i.wave_bin%i.pca%i.log' % (N_train, n_wave, n_pcas))\n",
        "\n",
        "            attributes = list([\n",
        "                    list(speculator.W_), \n",
        "                    list(speculator.b_), \n",
        "                    list(speculator.alphas_), \n",
        "                    list(speculator.betas_), \n",
        "                    speculator.pca_transform_matrix_,\n",
        "                    speculator.pca_shift_,\n",
        "                    speculator.pca_scale_,\n",
        "                    speculator.spectrum_shift_,\n",
        "                    speculator.spectrum_scale_,\n",
        "                    speculator.parameters_shift_, \n",
        "                    speculator.parameters_scale_,\n",
        "                    speculator.wavelengths])\n",
        "\n",
        "            # save attributes to file \n",
        "            f = open('DESI_complexdust_model.Ntrain%i.wave_bin%i.pca%i.log.pkl' % (N_train, n_wave, n_pcas), 'wb')\n",
        "            pickle.dump(attributes, f)\n",
        "            f.close()\n",
        "            print('Validation loss = %s' % str(best_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "learning rate = 0.001, batch size = 5000\n",
            "Validation loss = 0.016556678\n",
            "learning rate = 0.0001, batch size = 10000\n",
            "Validation loss = 0.0064228643\n",
            "learning rate = 1e-05, batch size = 50000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}