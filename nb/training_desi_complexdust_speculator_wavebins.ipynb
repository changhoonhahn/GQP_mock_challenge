{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_desi_complexdust_speculator_wavebins",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOpKKp93lHaZWg5PS1A829a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changhoonhahn/gqp_mc/blob/master/nb/training_desi_complexdust_speculator_wavebins.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_IHasbaJ3bu",
        "outputId": "4f39d016-aa59-4869-fa6d-36c16666864c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxG99o_9J7nF",
        "outputId": "0efb177d-d8ff-4fcb-9c7b-2ca23761292b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/drive/My\\ Drive/speculator_fork"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/speculator_fork\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XMBUjiqKNOy"
      },
      "source": [
        "import os \n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from speculator import SpectrumPCA\n",
        "from speculator import Speculator"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZJ8KvuRKOaC"
      },
      "source": [
        "# read DESI wavelength\n",
        "wave = np.load('wave_fsps.npy')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WMWkF1upSyq"
      },
      "source": [
        "n_wave = 0\n",
        "if n_wave == 0: \n",
        "    wave_bin = (wave < 4500) \n",
        "elif n_wave == 1: \n",
        "    wave_bin = (wave >= 4500) & (wave < 6500) \n",
        "elif n_wave == 2: \n",
        "    wave_bin = (wave >= 6500) "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZjNv5McKPaC"
      },
      "source": [
        "n_param = 10\n",
        "n_pcas  = 30 "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mTGmpPGKVX1",
        "outputId": "fa4e2852-f73d-49fa-e8d5-7948480af52e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# load trained PCA basis object\n",
        "print('training PCA bases') \n",
        "PCABasis = SpectrumPCA(\n",
        "        n_parameters=n_param,       # number of parameters\n",
        "        n_wavelengths=np.sum(wave_bin),       # number of wavelength values\n",
        "        n_pcas=n_pcas,              # number of pca coefficients to include in the basis \n",
        "        spectrum_filenames=None,  # list of filenames containing the (un-normalized) log spectra for training the PCA\n",
        "        parameter_filenames=[], # list of filenames containing the corresponding parameter values\n",
        "        parameter_selection=None) # pass an optional function that takes in parameter vector(s) and returns True/False for any extra parameter cuts we want to impose on the training sample (eg we may want to restrict the parameter ranges)\n",
        "PCABasis._load_from_file('DESI_complexdust.0_499.seed0.wave_bin%i.pca%i.hdf5' % (n_wave, n_pcas))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training PCA bases\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmeaPQSRP02T"
      },
      "source": [
        "_training_theta = np.load('DESI_complexdust.0_499.seed0.wave_bin%i.pca%i_parameters.npy' % (n_wave, n_pcas))\n",
        "_training_pca = np.load('DESI_complexdust.0_499.seed0.wave_bin%i.pca%i_pca.npy'% (n_wave, n_pcas))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7gmaXw8zGYa"
      },
      "source": [
        "N_train = int(2e6)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bht7PTNwzFe7"
      },
      "source": [
        "training_theta = tf.convert_to_tensor(_training_theta.astype(np.float32)[:N_train,:])\n",
        "training_pca = tf.convert_to_tensor(_training_pca.astype(np.float32)[:N_train,:])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkOJ_36YP4zD",
        "outputId": "a8233b34-2e85-4432-fdce-53f7c62a2296",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('training set size = %i' % training_pca.shape[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training set size = 2000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKCvnjiRKZ6I",
        "outputId": "c9190624-3c69-47ac-8e60-a25959411c8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# train Speculator \n",
        "speculator = Speculator(\n",
        "        n_parameters=n_param, # number of model parameters \n",
        "        wavelengths=wave, # array of wavelengths\n",
        "        pca_transform_matrix=PCABasis.pca_transform_matrix,\n",
        "        parameters_shift=PCABasis.parameters_shift, \n",
        "        parameters_scale=PCABasis.parameters_scale, \n",
        "        pca_shift=PCABasis.pca_shift, \n",
        "        pca_scale=PCABasis.pca_scale, \n",
        "        spectrum_shift=PCABasis.spectrum_shift, \n",
        "        spectrum_scale=PCABasis.spectrum_scale, \n",
        "        n_hidden=[256, 256, 256], # network architecture (list of hidden units per layer)\n",
        "        restore=False, \n",
        "        optimizer=tf.keras.optimizers.Adam()) # optimizer for model training\n",
        "\n",
        "# cooling schedule\n",
        "lr = [1e-3, 5e-3, 1e-4, 5e-5, 1e-5, 1e-6]\n",
        "batch_size = [1000, 5000, 10000, 50000, 100000, 1000000]#int(training_theta.shape[0])]\n",
        "gradient_accumulation_steps = [1, 1, 1, 1, 1, 1] # split the largest batch size into 10 when computing gradients to avoid memory overflow\n",
        "\n",
        "# early stopping set up\n",
        "patience = 40\n",
        "iepoch = 0 \n",
        "\n",
        "# save loss function\n",
        "_floss = os.path.join('DESI_complexdust_model.Ntrain%i.wave_bin%i.pca%i.loss.dat' % (N_train, n_wave, n_pcas))\n",
        "floss = open(_floss, 'w')\n",
        "floss.close()\n",
        "\n",
        "# train using cooling/heating schedule for lr/batch-size\n",
        "for i in range(len(lr)):\n",
        "    print('learning rate = ' + str(lr[i]) + ', batch size = ' + str(batch_size[i]))\n",
        "    # set learning rate\n",
        "    speculator.optimizer.lr = lr[i]\n",
        "\n",
        "    n_training = training_theta.shape[0]\n",
        "    # create iterable dataset (given batch size)\n",
        "    training_data = tf.data.Dataset.from_tensor_slices((training_theta, training_pca)).shuffle(n_training).batch(batch_size[i])\n",
        "\n",
        "    # set up training loss\n",
        "    training_loss   = [np.infty]\n",
        "    validation_loss = [np.infty]\n",
        "    best_loss       = np.infty\n",
        "    early_stopping_counter = 0\n",
        "\n",
        "    # loop over epochs\n",
        "    while early_stopping_counter < patience:\n",
        "\n",
        "        # loop over batches\n",
        "        for theta, pca in training_data:\n",
        "\n",
        "            # training step: check whether to accumulate gradients or not (only worth doing this for very large batch sizes)\n",
        "            if gradient_accumulation_steps[i] == 1:\n",
        "                loss = speculator.training_step(theta, pca)\n",
        "            else:\n",
        "                loss = speculator.training_step_with_accumulated_gradients(theta, pca, accumulation_steps=gradient_accumulation_steps[i])\n",
        "        \n",
        "        # compute validation loss at the end of the epoch\n",
        "        _loss = speculator.compute_loss(training_theta, training_pca).numpy()\n",
        "        validation_loss.append(_loss)\n",
        "        iepoch += 1\n",
        "\n",
        "        if (iepoch % 10) == 0: \n",
        "          print('%i: %i \\t %f \\t %f \\n' % (iepoch, batch_size[i], lr[i], _loss))\n",
        "        floss = open(_floss, 'a')\n",
        "        floss.write('%i \\t %f \\t %f \\n' % (batch_size[i], lr[i], _loss))\n",
        "        floss.close()\n",
        "\n",
        "        # early stopping condition\n",
        "        if validation_loss[-1] < best_loss:\n",
        "            best_loss = validation_loss[-1]\n",
        "            early_stopping_counter = 0\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "\n",
        "        if early_stopping_counter >= patience:\n",
        "            speculator.update_emulator_parameters()\n",
        "            speculator.save('_DESI_complexdust_model.Ntrain%i.wave_bin%i.pca%i.log' % (N_train, n_wave, n_pcas))\n",
        "\n",
        "            attributes = list([\n",
        "                    list(speculator.W_), \n",
        "                    list(speculator.b_), \n",
        "                    list(speculator.alphas_), \n",
        "                    list(speculator.betas_), \n",
        "                    speculator.pca_transform_matrix_,\n",
        "                    speculator.pca_shift_,\n",
        "                    speculator.pca_scale_,\n",
        "                    speculator.spectrum_shift_,\n",
        "                    speculator.spectrum_scale_,\n",
        "                    speculator.parameters_shift_, \n",
        "                    speculator.parameters_scale_,\n",
        "                    speculator.wavelengths])\n",
        "\n",
        "            # save attributes to file \n",
        "            f = open('DESI_complexdust_model.Ntrain%i.wave_bin%i.pca%i.log.pkl' % (N_train, n_wave, n_pcas), 'wb')\n",
        "            pickle.dump(attributes, f)\n",
        "            f.close()\n",
        "            print('Validation loss = %s' % str(best_loss))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "learning rate = 0.001, batch size = 1000\n",
            "10: 1000 \t 0.001000 \t 0.480812 \n",
            "\n",
            "20: 1000 \t 0.001000 \t 0.085200 \n",
            "\n",
            "30: 1000 \t 0.001000 \t 0.052687 \n",
            "\n",
            "40: 1000 \t 0.001000 \t 0.057793 \n",
            "\n",
            "50: 1000 \t 0.001000 \t 0.049154 \n",
            "\n",
            "60: 1000 \t 0.001000 \t 0.045988 \n",
            "\n",
            "70: 1000 \t 0.001000 \t 0.021228 \n",
            "\n",
            "80: 1000 \t 0.001000 \t 0.024973 \n",
            "\n",
            "90: 1000 \t 0.001000 \t 0.022469 \n",
            "\n",
            "100: 1000 \t 0.001000 \t 0.022788 \n",
            "\n",
            "110: 1000 \t 0.001000 \t 0.023429 \n",
            "\n",
            "120: 1000 \t 0.001000 \t 0.022393 \n",
            "\n",
            "130: 1000 \t 0.001000 \t 0.023096 \n",
            "\n",
            "140: 1000 \t 0.001000 \t 0.022073 \n",
            "\n",
            "150: 1000 \t 0.001000 \t 0.019851 \n",
            "\n",
            "160: 1000 \t 0.001000 \t 0.016678 \n",
            "\n",
            "170: 1000 \t 0.001000 \t 0.009950 \n",
            "\n",
            "180: 1000 \t 0.001000 \t 0.015045 \n",
            "\n",
            "190: 1000 \t 0.001000 \t 0.013135 \n",
            "\n",
            "200: 1000 \t 0.001000 \t 0.011120 \n",
            "\n",
            "210: 1000 \t 0.001000 \t 0.010170 \n",
            "\n",
            "220: 1000 \t 0.001000 \t 0.011216 \n",
            "\n",
            "230: 1000 \t 0.001000 \t 0.014271 \n",
            "\n",
            "240: 1000 \t 0.001000 \t 0.017022 \n",
            "\n",
            "Validation loss = 0.008159716\n",
            "learning rate = 0.005, batch size = 5000\n",
            "250: 5000 \t 0.005000 \t 0.053873 \n",
            "\n",
            "260: 5000 \t 0.005000 \t 0.025864 \n",
            "\n",
            "270: 5000 \t 0.005000 \t 0.041637 \n",
            "\n",
            "280: 5000 \t 0.005000 \t 0.026184 \n",
            "\n",
            "290: 5000 \t 0.005000 \t 0.030991 \n",
            "\n",
            "300: 5000 \t 0.005000 \t 0.017923 \n",
            "\n",
            "Validation loss = 0.016154367\n",
            "learning rate = 0.0001, batch size = 10000\n",
            "310: 10000 \t 0.000100 \t 0.005218 \n",
            "\n",
            "320: 10000 \t 0.000100 \t 0.004951 \n",
            "\n",
            "330: 10000 \t 0.000100 \t 0.004789 \n",
            "\n",
            "340: 10000 \t 0.000100 \t 0.004662 \n",
            "\n",
            "350: 10000 \t 0.000100 \t 0.004623 \n",
            "\n",
            "360: 10000 \t 0.000100 \t 0.004501 \n",
            "\n",
            "370: 10000 \t 0.000100 \t 0.004421 \n",
            "\n",
            "380: 10000 \t 0.000100 \t 0.004359 \n",
            "\n",
            "390: 10000 \t 0.000100 \t 0.004339 \n",
            "\n",
            "400: 10000 \t 0.000100 \t 0.004293 \n",
            "\n",
            "410: 10000 \t 0.000100 \t 0.004217 \n",
            "\n",
            "420: 10000 \t 0.000100 \t 0.004197 \n",
            "\n",
            "430: 10000 \t 0.000100 \t 0.004161 \n",
            "\n",
            "440: 10000 \t 0.000100 \t 0.004135 \n",
            "\n",
            "450: 10000 \t 0.000100 \t 0.004101 \n",
            "\n",
            "460: 10000 \t 0.000100 \t 0.004089 \n",
            "\n",
            "470: 10000 \t 0.000100 \t 0.004062 \n",
            "\n",
            "480: 10000 \t 0.000100 \t 0.004053 \n",
            "\n",
            "490: 10000 \t 0.000100 \t 0.004012 \n",
            "\n",
            "500: 10000 \t 0.000100 \t 0.004049 \n",
            "\n",
            "510: 10000 \t 0.000100 \t 0.003962 \n",
            "\n",
            "520: 10000 \t 0.000100 \t 0.003954 \n",
            "\n",
            "530: 10000 \t 0.000100 \t 0.003946 \n",
            "\n",
            "540: 10000 \t 0.000100 \t 0.003927 \n",
            "\n",
            "550: 10000 \t 0.000100 \t 0.003923 \n",
            "\n",
            "560: 10000 \t 0.000100 \t 0.003876 \n",
            "\n",
            "570: 10000 \t 0.000100 \t 0.003876 \n",
            "\n",
            "580: 10000 \t 0.000100 \t 0.003869 \n",
            "\n",
            "590: 10000 \t 0.000100 \t 0.003841 \n",
            "\n",
            "600: 10000 \t 0.000100 \t 0.003816 \n",
            "\n",
            "610: 10000 \t 0.000100 \t 0.003808 \n",
            "\n",
            "620: 10000 \t 0.000100 \t 0.003807 \n",
            "\n",
            "630: 10000 \t 0.000100 \t 0.003795 \n",
            "\n",
            "640: 10000 \t 0.000100 \t 0.003801 \n",
            "\n",
            "650: 10000 \t 0.000100 \t 0.003772 \n",
            "\n",
            "660: 10000 \t 0.000100 \t 0.003732 \n",
            "\n",
            "670: 10000 \t 0.000100 \t 0.003768 \n",
            "\n",
            "680: 10000 \t 0.000100 \t 0.003752 \n",
            "\n",
            "690: 10000 \t 0.000100 \t 0.003754 \n",
            "\n",
            "700: 10000 \t 0.000100 \t 0.003686 \n",
            "\n",
            "710: 10000 \t 0.000100 \t 0.003745 \n",
            "\n",
            "720: 10000 \t 0.000100 \t 0.003718 \n",
            "\n",
            "730: 10000 \t 0.000100 \t 0.003775 \n",
            "\n",
            "740: 10000 \t 0.000100 \t 0.003659 \n",
            "\n",
            "750: 10000 \t 0.000100 \t 0.003759 \n",
            "\n",
            "760: 10000 \t 0.000100 \t 0.003661 \n",
            "\n",
            "770: 10000 \t 0.000100 \t 0.003661 \n",
            "\n",
            "780: 10000 \t 0.000100 \t 0.003647 \n",
            "\n",
            "790: 10000 \t 0.000100 \t 0.003594 \n",
            "\n",
            "800: 10000 \t 0.000100 \t 0.003650 \n",
            "\n",
            "810: 10000 \t 0.000100 \t 0.003602 \n",
            "\n",
            "820: 10000 \t 0.000100 \t 0.003660 \n",
            "\n",
            "830: 10000 \t 0.000100 \t 0.003590 \n",
            "\n",
            "840: 10000 \t 0.000100 \t 0.003561 \n",
            "\n",
            "850: 10000 \t 0.000100 \t 0.003545 \n",
            "\n",
            "860: 10000 \t 0.000100 \t 0.003537 \n",
            "\n",
            "870: 10000 \t 0.000100 \t 0.003551 \n",
            "\n",
            "880: 10000 \t 0.000100 \t 0.003530 \n",
            "\n",
            "890: 10000 \t 0.000100 \t 0.003546 \n",
            "\n",
            "900: 10000 \t 0.000100 \t 0.003515 \n",
            "\n",
            "910: 10000 \t 0.000100 \t 0.003552 \n",
            "\n",
            "920: 10000 \t 0.000100 \t 0.003545 \n",
            "\n",
            "930: 10000 \t 0.000100 \t 0.003518 \n",
            "\n",
            "940: 10000 \t 0.000100 \t 0.003631 \n",
            "\n",
            "950: 10000 \t 0.000100 \t 0.003542 \n",
            "\n",
            "960: 10000 \t 0.000100 \t 0.003484 \n",
            "\n",
            "970: 10000 \t 0.000100 \t 0.003465 \n",
            "\n",
            "980: 10000 \t 0.000100 \t 0.003532 \n",
            "\n",
            "990: 10000 \t 0.000100 \t 0.003480 \n",
            "\n",
            "1000: 10000 \t 0.000100 \t 0.003489 \n",
            "\n",
            "1010: 10000 \t 0.000100 \t 0.003456 \n",
            "\n",
            "1020: 10000 \t 0.000100 \t 0.003452 \n",
            "\n",
            "1030: 10000 \t 0.000100 \t 0.003505 \n",
            "\n",
            "1040: 10000 \t 0.000100 \t 0.003440 \n",
            "\n",
            "1050: 10000 \t 0.000100 \t 0.003430 \n",
            "\n",
            "1060: 10000 \t 0.000100 \t 0.003434 \n",
            "\n",
            "1070: 10000 \t 0.000100 \t 0.003432 \n",
            "\n",
            "1080: 10000 \t 0.000100 \t 0.003440 \n",
            "\n",
            "1090: 10000 \t 0.000100 \t 0.003412 \n",
            "\n",
            "1100: 10000 \t 0.000100 \t 0.003408 \n",
            "\n",
            "1110: 10000 \t 0.000100 \t 0.003393 \n",
            "\n",
            "1120: 10000 \t 0.000100 \t 0.003400 \n",
            "\n",
            "1130: 10000 \t 0.000100 \t 0.003412 \n",
            "\n",
            "1140: 10000 \t 0.000100 \t 0.003392 \n",
            "\n",
            "1150: 10000 \t 0.000100 \t 0.003376 \n",
            "\n",
            "1160: 10000 \t 0.000100 \t 0.003382 \n",
            "\n",
            "1170: 10000 \t 0.000100 \t 0.003415 \n",
            "\n",
            "1180: 10000 \t 0.000100 \t 0.003358 \n",
            "\n",
            "1190: 10000 \t 0.000100 \t 0.003352 \n",
            "\n",
            "1200: 10000 \t 0.000100 \t 0.003365 \n",
            "\n",
            "1210: 10000 \t 0.000100 \t 0.003393 \n",
            "\n",
            "1220: 10000 \t 0.000100 \t 0.003342 \n",
            "\n",
            "1230: 10000 \t 0.000100 \t 0.003365 \n",
            "\n",
            "1240: 10000 \t 0.000100 \t 0.003320 \n",
            "\n",
            "1250: 10000 \t 0.000100 \t 0.003369 \n",
            "\n",
            "1260: 10000 \t 0.000100 \t 0.003321 \n",
            "\n",
            "1270: 10000 \t 0.000100 \t 0.003351 \n",
            "\n",
            "1280: 10000 \t 0.000100 \t 0.003372 \n",
            "\n",
            "1290: 10000 \t 0.000100 \t 0.003306 \n",
            "\n",
            "1300: 10000 \t 0.000100 \t 0.003330 \n",
            "\n",
            "1310: 10000 \t 0.000100 \t 0.003365 \n",
            "\n",
            "1320: 10000 \t 0.000100 \t 0.003314 \n",
            "\n",
            "1330: 10000 \t 0.000100 \t 0.003327 \n",
            "\n",
            "1340: 10000 \t 0.000100 \t 0.003289 \n",
            "\n",
            "1350: 10000 \t 0.000100 \t 0.003326 \n",
            "\n",
            "1360: 10000 \t 0.000100 \t 0.003333 \n",
            "\n",
            "1370: 10000 \t 0.000100 \t 0.003330 \n",
            "\n",
            "1380: 10000 \t 0.000100 \t 0.003401 \n",
            "\n",
            "1390: 10000 \t 0.000100 \t 0.003299 \n",
            "\n",
            "1400: 10000 \t 0.000100 \t 0.003300 \n",
            "\n",
            "1410: 10000 \t 0.000100 \t 0.003275 \n",
            "\n",
            "1420: 10000 \t 0.000100 \t 0.003273 \n",
            "\n",
            "1430: 10000 \t 0.000100 \t 0.003289 \n",
            "\n",
            "1440: 10000 \t 0.000100 \t 0.003313 \n",
            "\n",
            "1450: 10000 \t 0.000100 \t 0.003262 \n",
            "\n",
            "1460: 10000 \t 0.000100 \t 0.003260 \n",
            "\n",
            "1470: 10000 \t 0.000100 \t 0.003247 \n",
            "\n",
            "1480: 10000 \t 0.000100 \t 0.003326 \n",
            "\n",
            "1490: 10000 \t 0.000100 \t 0.003223 \n",
            "\n",
            "1500: 10000 \t 0.000100 \t 0.003294 \n",
            "\n",
            "1510: 10000 \t 0.000100 \t 0.003225 \n",
            "\n",
            "1520: 10000 \t 0.000100 \t 0.003239 \n",
            "\n",
            "1530: 10000 \t 0.000100 \t 0.003287 \n",
            "\n",
            "1540: 10000 \t 0.000100 \t 0.003275 \n",
            "\n",
            "1550: 10000 \t 0.000100 \t 0.003230 \n",
            "\n",
            "1560: 10000 \t 0.000100 \t 0.003225 \n",
            "\n",
            "1570: 10000 \t 0.000100 \t 0.003307 \n",
            "\n",
            "1580: 10000 \t 0.000100 \t 0.003258 \n",
            "\n",
            "1590: 10000 \t 0.000100 \t 0.003262 \n",
            "\n",
            "1600: 10000 \t 0.000100 \t 0.003224 \n",
            "\n",
            "1610: 10000 \t 0.000100 \t 0.003205 \n",
            "\n",
            "1620: 10000 \t 0.000100 \t 0.003266 \n",
            "\n",
            "1630: 10000 \t 0.000100 \t 0.003187 \n",
            "\n",
            "1640: 10000 \t 0.000100 \t 0.003201 \n",
            "\n",
            "1650: 10000 \t 0.000100 \t 0.003174 \n",
            "\n",
            "1660: 10000 \t 0.000100 \t 0.003186 \n",
            "\n",
            "1670: 10000 \t 0.000100 \t 0.003184 \n",
            "\n",
            "1680: 10000 \t 0.000100 \t 0.003202 \n",
            "\n",
            "1690: 10000 \t 0.000100 \t 0.003165 \n",
            "\n",
            "1700: 10000 \t 0.000100 \t 0.003174 \n",
            "\n",
            "1710: 10000 \t 0.000100 \t 0.003161 \n",
            "\n",
            "1720: 10000 \t 0.000100 \t 0.003164 \n",
            "\n",
            "1730: 10000 \t 0.000100 \t 0.003219 \n",
            "\n",
            "1740: 10000 \t 0.000100 \t 0.003150 \n",
            "\n",
            "1750: 10000 \t 0.000100 \t 0.003160 \n",
            "\n",
            "1760: 10000 \t 0.000100 \t 0.003163 \n",
            "\n",
            "Validation loss = 0.003137875\n",
            "learning rate = 5e-05, batch size = 50000\n",
            "1770: 50000 \t 0.000050 \t 0.003089 \n",
            "\n",
            "1780: 50000 \t 0.000050 \t 0.003091 \n",
            "\n",
            "1790: 50000 \t 0.000050 \t 0.003091 \n",
            "\n",
            "1800: 50000 \t 0.000050 \t 0.003095 \n",
            "\n",
            "Validation loss = 0.0030888368\n",
            "learning rate = 1e-05, batch size = 100000\n",
            "1810: 100000 \t 0.000010 \t 0.003083 \n",
            "\n",
            "1820: 100000 \t 0.000010 \t 0.003083 \n",
            "\n",
            "1830: 100000 \t 0.000010 \t 0.003083 \n",
            "\n",
            "1840: 100000 \t 0.000010 \t 0.003083 \n",
            "\n",
            "1850: 100000 \t 0.000010 \t 0.003083 \n",
            "\n",
            "1860: 100000 \t 0.000010 \t 0.003083 \n",
            "\n",
            "1870: 100000 \t 0.000010 \t 0.003083 \n",
            "\n",
            "1880: 100000 \t 0.000010 \t 0.003083 \n",
            "\n",
            "1890: 100000 \t 0.000010 \t 0.003083 \n",
            "\n",
            "1900: 100000 \t 0.000010 \t 0.003084 \n",
            "\n",
            "1910: 100000 \t 0.000010 \t 0.003083 \n",
            "\n",
            "1920: 100000 \t 0.000010 \t 0.003084 \n",
            "\n",
            "1930: 100000 \t 0.000010 \t 0.003083 \n",
            "\n",
            "1940: 100000 \t 0.000010 \t 0.003083 \n",
            "\n",
            "1950: 100000 \t 0.000010 \t 0.003083 \n",
            "\n",
            "1960: 100000 \t 0.000010 \t 0.003085 \n",
            "\n",
            "1970: 100000 \t 0.000010 \t 0.003083 \n",
            "\n",
            "1980: 100000 \t 0.000010 \t 0.003083 \n",
            "\n",
            "1990: 100000 \t 0.000010 \t 0.003083 \n",
            "\n",
            "2000: 100000 \t 0.000010 \t 0.003083 \n",
            "\n",
            "2010: 100000 \t 0.000010 \t 0.003083 \n",
            "\n",
            "2020: 100000 \t 0.000010 \t 0.003083 \n",
            "\n",
            "2030: 100000 \t 0.000010 \t 0.003083 \n",
            "\n",
            "2040: 100000 \t 0.000010 \t 0.003084 \n",
            "\n",
            "2050: 100000 \t 0.000010 \t 0.003081 \n",
            "\n",
            "2060: 100000 \t 0.000010 \t 0.003081 \n",
            "\n",
            "2070: 100000 \t 0.000010 \t 0.003081 \n",
            "\n",
            "2080: 100000 \t 0.000010 \t 0.003081 \n",
            "\n",
            "Validation loss = 0.0030808148\n",
            "learning rate = 1e-06, batch size = 1000000\n",
            "2090: 1000000 \t 0.000001 \t 0.003082 \n",
            "\n",
            "2100: 1000000 \t 0.000001 \t 0.003079 \n",
            "\n",
            "2110: 1000000 \t 0.000001 \t 0.003079 \n",
            "\n",
            "2120: 1000000 \t 0.000001 \t 0.003079 \n",
            "\n",
            "2130: 1000000 \t 0.000001 \t 0.003079 \n",
            "\n",
            "2140: 1000000 \t 0.000001 \t 0.003079 \n",
            "\n",
            "2150: 1000000 \t 0.000001 \t 0.003079 \n",
            "\n",
            "2160: 1000000 \t 0.000001 \t 0.003079 \n",
            "\n",
            "2170: 1000000 \t 0.000001 \t 0.003079 \n",
            "\n",
            "2180: 1000000 \t 0.000001 \t 0.003079 \n",
            "\n",
            "2190: 1000000 \t 0.000001 \t 0.003079 \n",
            "\n",
            "2200: 1000000 \t 0.000001 \t 0.003079 \n",
            "\n",
            "2210: 1000000 \t 0.000001 \t 0.003079 \n",
            "\n",
            "2220: 1000000 \t 0.000001 \t 0.003079 \n",
            "\n",
            "2230: 1000000 \t 0.000001 \t 0.003079 \n",
            "\n",
            "2240: 1000000 \t 0.000001 \t 0.003079 \n",
            "\n",
            "2250: 1000000 \t 0.000001 \t 0.003079 \n",
            "\n",
            "2260: 1000000 \t 0.000001 \t 0.003079 \n",
            "\n",
            "2270: 1000000 \t 0.000001 \t 0.003079 \n",
            "\n",
            "2280: 1000000 \t 0.000001 \t 0.003079 \n",
            "\n",
            "2290: 1000000 \t 0.000001 \t 0.003079 \n",
            "\n",
            "2300: 1000000 \t 0.000001 \t 0.003079 \n",
            "\n",
            "2310: 1000000 \t 0.000001 \t 0.003079 \n",
            "\n",
            "2320: 1000000 \t 0.000001 \t 0.003079 \n",
            "\n",
            "2330: 1000000 \t 0.000001 \t 0.003079 \n",
            "\n",
            "2340: 1000000 \t 0.000001 \t 0.003079 \n",
            "\n",
            "Validation loss = 0.0030791888\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcbXIKViJKHl"
      },
      "source": [
        "loss = np.loadtxt('DESI_complexdust_model.Ntrain%i.wave_bin%i.pca%i.loss.dat' % (N_train, n_wave, n_pcas))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afkB5AJ5JXkt",
        "outputId": "28debe6c-67fa-471f-b12c-e556b5e4ccba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "fig = plt.figure(figsize=(10,5))\n",
        "sub = fig.add_subplot(111)\n",
        "for batchsize in np.unique(loss[:,0]):\n",
        "  isbatch = (loss[:,0] == batchsize)\n",
        "  sub.plot(np.arange(loss.shape[0])[isbatch], loss[:,2][isbatch], label='%i' % batchsize)\n",
        "sub.legend(loc='upper right')\n",
        "sub.set_yscale('log')\n",
        "sub.set_ylim(3e-3, 0.1)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.003, 0.1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FulL4hm4JYG6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}